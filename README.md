# Flash Attention Wheels

Pre-built wheels for Flash Attention 2 & 3. Skip the 30+ minute compilation.

## Features

- Search and filter by CUDA, Python, PyTorch, and Platform
- One-click copy for `pip` and `uv` install commands
- Direct download links
- Flash Attention 2 & 3 support

## Supported Configurations

**Flash Attention 2**
- CUDA: 11.8, 12.1, 12.2, 12.3, 12.4, 12.6
- PyTorch: 2.0 - 2.10
- Python: 3.8 - 3.12
- Platforms: Linux x86_64, Linux ARM64, Windows

**Flash Attention 3**
- CUDA: 12.6, 12.8, 12.9, 13.0
- PyTorch: 2.8 - 2.10
- Python: 3.10 - 3.12
- Platforms: Linux x86_64, Linux ARM64, Windows

## Sources

- [flashattn.dev](https://flashattn.dev/)
- [Flash Attention 3 Wheels](https://windreamer.github.io/flash-attention3-wheels/)
- [mjun0812/flash-attention-prebuild-wheels](https://github.com/mjun0812/flash-attention-prebuild-wheels)
- [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)

## License

MIT
