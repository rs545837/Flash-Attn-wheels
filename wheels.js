// Flash Attention 2 Wheels
const flashAttn2 = [
  // v2.6.3 - CUDA 12.6
  { v: "2.6.3", cuda: "12.6", torch: "2.10", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.10-cp310-cp310-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.10", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.10-cp311-cp311-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.10", py: "3.12", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.10-cp312-cp312-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.9", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.9-cp310-cp310-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.9", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.9-cp311-cp311-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.9", py: "3.12", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.9-cp312-cp312-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.8", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.8-cp310-cp310-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.8", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.8-cp311-cp311-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.8", py: "3.12", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl", size: "177 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.5", py: "3.10", os: "linux_aarch64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.5-cp310-cp310-linux_aarch64.whl", size: "175 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.5", py: "3.11", os: "linux_aarch64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.5-cp311-cp311-linux_aarch64.whl", size: "175 MB" },
  { v: "2.6.3", cuda: "12.6", torch: "2.5", py: "3.12", os: "linux_aarch64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu126torch2.5-cp312-cp312-linux_aarch64.whl", size: "175 MB" },
  // v2.6.3 - CUDA 12.4
  { v: "2.6.3", cuda: "12.4", torch: "2.6", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.6-cp310-cp310-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.6", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.6-cp311-cp311-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.6", py: "3.12", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.6-cp312-cp312-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.5", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.5-cp310-cp310-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.5", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.5-cp311-cp311-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.5", py: "3.12", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.5-cp312-cp312-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.4", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.4-cp310-cp310-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.4", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.4-cp311-cp311-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.4", py: "3.12", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.4-cp312-cp312-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.5", py: "3.10", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu124torch2.5-cp310-cp310-win_amd64.whl", size: "165 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.5", py: "3.11", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu124torch2.5-cp311-cp311-win_amd64.whl", size: "165 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.5", py: "3.12", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu124torch2.5-cp312-cp312-win_amd64.whl", size: "165 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.4", py: "3.10", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu124torch2.4-cp310-cp310-win_amd64.whl", size: "165 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.4", py: "3.11", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu124torch2.4-cp311-cp311-win_amd64.whl", size: "165 MB" },
  { v: "2.6.3", cuda: "12.4", torch: "2.4", py: "3.12", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu124torch2.4-cp312-cp312-win_amd64.whl", size: "165 MB" },
  // v2.6.3 - CUDA 12.3
  { v: "2.6.3", cuda: "12.3", torch: "2.3", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu123torch2.3-cp310-cp310-linux_x86_64.whl", size: "172 MB" },
  { v: "2.6.3", cuda: "12.3", torch: "2.3", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu123torch2.3-cp311-cp311-linux_x86_64.whl", size: "172 MB" },
  { v: "2.6.3", cuda: "12.3", torch: "2.3", py: "3.12", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu123torch2.3-cp312-cp312-linux_x86_64.whl", size: "172 MB" },
  // v2.6.3 - CUDA 12.2
  { v: "2.6.3", cuda: "12.2", torch: "2.2", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu122torch2.2-cp310-cp310-linux_x86_64.whl", size: "170 MB" },
  { v: "2.6.3", cuda: "12.2", torch: "2.2", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu122torch2.2-cp311-cp311-linux_x86_64.whl", size: "170 MB" },
  { v: "2.6.3", cuda: "12.2", torch: "2.2", py: "3.12", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu122torch2.2-cp312-cp312-linux_x86_64.whl", size: "170 MB" },
  // v2.6.3 - CUDA 12.1
  { v: "2.6.3", cuda: "12.1", torch: "2.1", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu121torch2.1-cp310-cp310-linux_x86_64.whl", size: "168 MB" },
  { v: "2.6.3", cuda: "12.1", torch: "2.1", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu121torch2.1-cp311-cp311-linux_x86_64.whl", size: "168 MB" },
  { v: "2.6.3", cuda: "12.1", torch: "2.0", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu121torch2.0-cp310-cp310-linux_x86_64.whl", size: "168 MB" },
  { v: "2.6.3", cuda: "12.1", torch: "2.0", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu121torch2.0-cp311-cp311-linux_x86_64.whl", size: "168 MB" },
  // v2.6.3 - CUDA 11.8
  { v: "2.6.3", cuda: "11.8", torch: "2.4", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.4-cp310-cp310-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.4", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.4-cp311-cp311-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.3", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.3-cp310-cp310-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.3", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.3-cp311-cp311-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.2", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.2-cp310-cp310-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.2", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.2-cp311-cp311-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.1", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.1-cp310-cp310-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.1", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.1-cp311-cp311-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.0", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.0-cp310-cp310-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.0", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.0-cp311-cp311-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.4", py: "3.10", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu118torch2.4-cp310-cp310-win_amd64.whl", size: "158 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.4", py: "3.11", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu118torch2.4-cp311-cp311-win_amd64.whl", size: "158 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.4", py: "3.12", os: "win_amd64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.13/flash_attn-2.6.3+cu118torch2.4-cp312-cp312-win_amd64.whl", size: "158 MB" },
  // Python 3.9 support
  { v: "2.6.3", cuda: "12.4", torch: "2.4", py: "3.9", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu124torch2.4-cp39-cp39-linux_x86_64.whl", size: "174 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.3", py: "3.9", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.3-cp39-cp39-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.2", py: "3.9", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.2-cp39-cp39-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.1", py: "3.9", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.1-cp39-cp39-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.0", py: "3.8", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.0-cp38-cp38-linux_x86_64.whl", size: "166 MB" },
  { v: "2.6.3", cuda: "11.8", torch: "2.0", py: "3.9", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.6.3+cu118torch2.0-cp39-cp39-linux_x86_64.whl", size: "166 MB" },
  // Older v2.5.9
  { v: "2.5.9", cuda: "12.4", torch: "2.4", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.6/flash_attn-2.5.9+cu124torch2.4-cp310-cp310-linux_x86_64.whl", size: "172 MB" },
  { v: "2.5.9", cuda: "12.4", torch: "2.4", py: "3.11", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.6/flash_attn-2.5.9+cu124torch2.4-cp311-cp311-linux_x86_64.whl", size: "172 MB" },
  { v: "2.5.9", cuda: "12.1", torch: "2.3", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.6/flash_attn-2.5.9+cu121torch2.3-cp310-cp310-linux_x86_64.whl", size: "168 MB" },
  { v: "2.5.9", cuda: "11.8", torch: "2.3", py: "3.10", os: "linux_x86_64", url: "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.6/flash_attn-2.5.9+cu118torch2.3-cp310-cp310-linux_x86_64.whl", size: "164 MB" },
];

// Flash Attention 3 Wheels
const flashAttn3 = [
  // CUDA 13.0
  { v: "3.0.0", cuda: "13.0", torch: "2.10", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu130/torch2.10.0/flash_attn_3-3.0.0+cu130torch2.10-cp311-cp311-linux_x86_64.whl", size: "~180 MB" },
  { v: "3.0.0", cuda: "13.0", torch: "2.10", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu130/torch2.10.0/flash_attn_3-3.0.0+cu130torch2.10-cp312-cp312-linux_x86_64.whl", size: "~180 MB" },
  { v: "3.0.0", cuda: "13.0", torch: "2.10", py: "3.11", os: "linux_aarch64", url: "https://windreamer.github.io/flash-attention3-wheels/cu130/torch2.10.0/flash_attn_3-3.0.0+cu130torch2.10-cp311-cp311-linux_aarch64.whl", size: "~180 MB" },
  { v: "3.0.0", cuda: "13.0", torch: "2.9", py: "3.10", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu130/torch2.9.1/flash_attn_3-3.0.0+cu130torch2.9.1-cp310-cp310-linux_x86_64.whl", size: "~180 MB" },
  { v: "3.0.0", cuda: "13.0", torch: "2.9", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu130/torch2.9.1/flash_attn_3-3.0.0+cu130torch2.9.1-cp311-cp311-linux_x86_64.whl", size: "~180 MB" },
  { v: "3.0.0", cuda: "13.0", torch: "2.9", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu130/torch2.9.1/flash_attn_3-3.0.0+cu130torch2.9.1-cp312-cp312-linux_x86_64.whl", size: "~180 MB" },
  // CUDA 12.9
  { v: "3.0.0", cuda: "12.9", torch: "2.10", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.10.0/flash_attn_3-3.0.0+cu129torch2.10-cp311-cp311-linux_x86_64.whl", size: "~178 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.10", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.10.0/flash_attn_3-3.0.0+cu129torch2.10-cp312-cp312-linux_x86_64.whl", size: "~178 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.9", py: "3.10", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.9.1/flash_attn_3-3.0.0+cu129torch2.9.1-cp310-cp310-linux_x86_64.whl", size: "~178 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.9", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.9.1/flash_attn_3-3.0.0+cu129torch2.9.1-cp311-cp311-linux_x86_64.whl", size: "~178 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.9", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.9.1/flash_attn_3-3.0.0+cu129torch2.9.1-cp312-cp312-linux_x86_64.whl", size: "~178 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.9", py: "3.10", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.9.0/flash_attn_3-3.0.0+cu129torch2.9-cp310-cp310-win_amd64.whl", size: "~170 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.9", py: "3.11", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.9.0/flash_attn_3-3.0.0+cu129torch2.9-cp311-cp311-win_amd64.whl", size: "~170 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.9", py: "3.12", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.9.0/flash_attn_3-3.0.0+cu129torch2.9-cp312-cp312-win_amd64.whl", size: "~170 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.8", py: "3.10", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.8.0/flash_attn_3-3.0.0+cu129torch2.8-cp310-cp310-linux_x86_64.whl", size: "~178 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.8", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.8.0/flash_attn_3-3.0.0+cu129torch2.8-cp311-cp311-linux_x86_64.whl", size: "~178 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.8", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.8.0/flash_attn_3-3.0.0+cu129torch2.8-cp312-cp312-linux_x86_64.whl", size: "~178 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.8", py: "3.10", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.8.0/flash_attn_3-3.0.0+cu129torch2.8-cp310-cp310-win_amd64.whl", size: "~170 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.8", py: "3.11", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.8.0/flash_attn_3-3.0.0+cu129torch2.8-cp311-cp311-win_amd64.whl", size: "~170 MB" },
  { v: "3.0.0", cuda: "12.9", torch: "2.8", py: "3.12", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu129/torch2.8.0/flash_attn_3-3.0.0+cu129torch2.8-cp312-cp312-win_amd64.whl", size: "~170 MB" },
  // CUDA 12.8
  { v: "3.0.0", cuda: "12.8", torch: "2.9", py: "3.10", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.9.1/flash_attn_3-3.0.0+cu128torch2.9.1-cp310-cp310-linux_x86_64.whl", size: "~176 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.9", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.9.1/flash_attn_3-3.0.0+cu128torch2.9.1-cp311-cp311-linux_x86_64.whl", size: "~176 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.9", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.9.1/flash_attn_3-3.0.0+cu128torch2.9.1-cp312-cp312-linux_x86_64.whl", size: "~176 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.9", py: "3.10", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.9.1/flash_attn_3-3.0.0+cu128torch2.9.1-cp310-cp310-win_amd64.whl", size: "~168 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.9", py: "3.11", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.9.1/flash_attn_3-3.0.0+cu128torch2.9.1-cp311-cp311-win_amd64.whl", size: "~168 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.9", py: "3.12", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.9.1/flash_attn_3-3.0.0+cu128torch2.9.1-cp312-cp312-win_amd64.whl", size: "~168 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.8", py: "3.10", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.8.0/flash_attn_3-3.0.0+cu128torch2.8-cp310-cp310-linux_x86_64.whl", size: "~176 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.8", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.8.0/flash_attn_3-3.0.0+cu128torch2.8-cp311-cp311-linux_x86_64.whl", size: "~176 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.8", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.8.0/flash_attn_3-3.0.0+cu128torch2.8-cp312-cp312-linux_x86_64.whl", size: "~176 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.8", py: "3.10", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.8.0/flash_attn_3-3.0.0+cu128torch2.8-cp310-cp310-win_amd64.whl", size: "~168 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.8", py: "3.11", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.8.0/flash_attn_3-3.0.0+cu128torch2.8-cp311-cp311-win_amd64.whl", size: "~168 MB" },
  { v: "3.0.0", cuda: "12.8", torch: "2.8", py: "3.12", os: "win_amd64", url: "https://windreamer.github.io/flash-attention3-wheels/cu128/torch2.8.0/flash_attn_3-3.0.0+cu128torch2.8-cp312-cp312-win_amd64.whl", size: "~168 MB" },
  // CUDA 12.6
  { v: "3.0.0", cuda: "12.6", torch: "2.9", py: "3.10", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu126/torch2.9.1/flash_attn_3-3.0.0+cu126torch2.9.1-cp310-cp310-linux_x86_64.whl", size: "~174 MB" },
  { v: "3.0.0", cuda: "12.6", torch: "2.9", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu126/torch2.9.1/flash_attn_3-3.0.0+cu126torch2.9.1-cp311-cp311-linux_x86_64.whl", size: "~174 MB" },
  { v: "3.0.0", cuda: "12.6", torch: "2.9", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu126/torch2.9.1/flash_attn_3-3.0.0+cu126torch2.9.1-cp312-cp312-linux_x86_64.whl", size: "~174 MB" },
  { v: "3.0.0", cuda: "12.6", torch: "2.8", py: "3.10", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu126/torch2.8.0/flash_attn_3-3.0.0+cu126torch2.8-cp310-cp310-linux_x86_64.whl", size: "~174 MB" },
  { v: "3.0.0", cuda: "12.6", torch: "2.8", py: "3.11", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu126/torch2.8.0/flash_attn_3-3.0.0+cu126torch2.8-cp311-cp311-linux_x86_64.whl", size: "~174 MB" },
  { v: "3.0.0", cuda: "12.6", torch: "2.8", py: "3.12", os: "linux_x86_64", url: "https://windreamer.github.io/flash-attention3-wheels/cu126/torch2.8.0/flash_attn_3-3.0.0+cu126torch2.8-cp312-cp312-linux_x86_64.whl", size: "~174 MB" },
];

// Export combined
const allWheels = { flashAttn2, flashAttn3 };
